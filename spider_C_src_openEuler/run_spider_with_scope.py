import time
import subprocess
import random
import pandas as pd


# 源文件路径和目标文件路径
source_file = 'github_C.csv'

scope = [i for i in range(4, 14)]


df = pd.read_csv(source_file)  # step1得到的

# urls = [df.iloc[i]['repoLink']+"/commits" for i in range(len(df))] openeuler
#119目前爬到9，下一个【10，15】
#476目前爬到200，下一个【200：210】
urls = [link + "/commits" for link in df['repo_url'].tolist()[190:200]] # 指定爬取哪几个仓库
#urls =['https://github.com/obsproject/obs-studio/commits/master','https://github.com/git/git/commits/master']
#urls=['https://github.com/bminor/binutils-gdb/commits/master']
#urls=['https://github.com/vantoman/kernel_xiaomi_sm6150/commits/14']
#https://github.com/vantoman/kernel_xiaomi_sm6150/commit/81b4b98ae484f11d97b3d5b8e88d916b74055b78
for url in urls:
    print("starting..........  ", url)
    subprocess.run(['scrapy', 'crawl', 'C_openEuler_Bykey', '-a', f'start_url={url}', '-o', 'github_C_all.csv'])#保存在github_C_119.csv
    sleep_time = random.choice(scope)
    print("接下来会休眠: ", sleep_time, "s")
    time.sleep(sleep_time)

#df = pd.read_csv('patchsInfo.csv')
#idx = df[df['commit_url'] == 'commit_url'].index
#print(idx)
#df_process = df.drop(index=idx)   # 删除重复的列名字
#df_process.to_csv('patchsInfo.csv', index=False)